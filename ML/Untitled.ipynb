{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f7ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ff92fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40320 chess moves.\n"
     ]
    }
   ],
   "source": [
    "pieces = [\"pawn\", \"knight\", \"bishop\", \"rook\", \"queen\", \"king\", \"porn\", \"on\", \"night\", \"ruk\"]\n",
    "columns = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "rows = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "\n",
    "def generate_chess_moves():\n",
    "    data = []\n",
    "    for piece in pieces:\n",
    "        for from_col in columns:\n",
    "            for from_row in rows:\n",
    "                for to_col in columns:\n",
    "                    for to_row in rows:\n",
    "                        if from_col != to_col or from_row != to_row:  # to avoid moves where piece doesn't move\n",
    "                            text = f\"move {piece} from {from_col}{from_row} to {to_col}{to_row}\"\n",
    "                            data.append({\"text\": text, \"piece\": piece, \"from\": f\"{from_col}{from_row}\", \"to\": f\"{to_col}{to_row}\"})\n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "data = generate_chess_moves()\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('large_chess_moves.json', 'w') as f:\n",
    "    json.dump(data, f)\n",
    "\n",
    "print(f\"Generated {len(data)} chess moves.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35433848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1260/1260 [==============================] - 17s 11ms/step - loss: 3.9031 - piece_output_loss: 0.2399 - from_output_loss: 2.2295 - to_output_loss: 1.4337 - piece_output_accuracy: 0.9338 - from_output_accuracy: 0.4374 - to_output_accuracy: 0.6404\n",
      "Epoch 2/10\n",
      "1260/1260 [==============================] - 14s 11ms/step - loss: 0.1335 - piece_output_loss: 0.0162 - from_output_loss: 0.0679 - to_output_loss: 0.0495 - piece_output_accuracy: 0.9984 - from_output_accuracy: 0.9961 - to_output_accuracy: 0.9971\n",
      "Epoch 3/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 0.0189 - piece_output_loss: 0.0025 - from_output_loss: 0.0089 - to_output_loss: 0.0075 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 0.0061 - piece_output_loss: 8.4461e-04 - from_output_loss: 0.0028 - to_output_loss: 0.0024 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 0.0022 - piece_output_loss: 3.1441e-04 - from_output_loss: 0.0010 - to_output_loss: 9.1054e-04 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 0.0010 - piece_output_loss: 1.4650e-04 - from_output_loss: 4.4896e-04 - to_output_loss: 4.1150e-04 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 4.8793e-04 - piece_output_loss: 6.9528e-05 - from_output_loss: 2.1530e-04 - to_output_loss: 2.0311e-04 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1260/1260 [==============================] - 15s 12ms/step - loss: 2.4108e-04 - piece_output_loss: 3.5428e-05 - from_output_loss: 1.0608e-04 - to_output_loss: 9.9580e-05 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1260/1260 [==============================] - 16s 12ms/step - loss: 1.2220e-04 - piece_output_loss: 1.8290e-05 - from_output_loss: 5.3240e-05 - to_output_loss: 5.0672e-05 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1260/1260 [==============================] - 16s 12ms/step - loss: 6.1798e-05 - piece_output_loss: 9.2986e-06 - from_output_loss: 2.6843e-05 - to_output_loss: 2.5656e-05 - piece_output_accuracy: 1.0000 - from_output_accuracy: 1.0000 - to_output_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load data\n",
    "with open('large_chess_moves.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare data\n",
    "texts = [item['text'] for item in data]\n",
    "pieces = [item['piece'] for item in data]\n",
    "from_positions = [item['from'] for item in data]\n",
    "to_positions = [item['to'] for item in data]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Simplified target encoding\n",
    "piece_tokenizer = Tokenizer()\n",
    "piece_tokenizer.fit_on_texts(pieces)\n",
    "y_pieces = np.array(piece_tokenizer.texts_to_sequences(pieces)).flatten()\n",
    "\n",
    "position_tokenizer = Tokenizer()\n",
    "position_tokenizer.fit_on_texts(from_positions + to_positions)\n",
    "y_from_positions = np.array(position_tokenizer.texts_to_sequences(from_positions)).flatten()\n",
    "y_to_positions = np.array(position_tokenizer.texts_to_sequences(to_positions)).flatten()\n",
    "\n",
    "# Reshape the targets to be compatible with model output\n",
    "y_pieces = y_pieces.reshape(-1, 1)\n",
    "y_from_positions = y_from_positions.reshape(-1, 1)\n",
    "y_to_positions = y_to_positions.reshape(-1, 1)\n",
    "\n",
    "# Build the model using the functional API\n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=64, input_length=max_length)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "lstm_layer = Bidirectional(LSTM(64))(lstm_layer)\n",
    "\n",
    "dense_layer = Dense(64, activation='relu')(lstm_layer)\n",
    "\n",
    "piece_output = Dense(len(piece_tokenizer.word_index) + 1, activation='softmax', name='piece_output')(dense_layer)\n",
    "from_output = Dense(len(position_tokenizer.word_index) + 1, activation='softmax', name='from_output')(dense_layer)\n",
    "to_output = Dense(len(position_tokenizer.word_index) + 1, activation='softmax', name='to_output')(dense_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[piece_output, from_output, to_output])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, [y_pieces, y_from_positions, y_to_positions], epochs=10)\n",
    "\n",
    "# Save the model and tokenizers\n",
    "model.save('chess_model.h5')\n",
    "with open('tokenizer.json', 'w') as f:\n",
    "    f.write(tokenizer.to_json())\n",
    "with open('piece_tokenizer.json', 'w') as f:\n",
    "    f.write(piece_tokenizer.to_json())\n",
    "with open('position_tokenizer.json', 'w') as f:\n",
    "    f.write(position_tokenizer.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b00e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
